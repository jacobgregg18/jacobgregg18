{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPblK7k6W+j5BEPjWyKGnVy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacobgregg18/jacobgregg18/blob/main/LSTMFile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloads"
      ],
      "metadata": {
        "id": "WbAs9QaRSIwB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMlXld_rR1R1",
        "outputId": "09b6dcf9-7736-4420-e269-4d311acb4727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import natsort\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.colors import ListedColormap\n",
        "from scipy.spatial import KDTree\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.amp import GradScaler\n",
        "from torch.amp import autocast\n",
        "import gc\n",
        "import time\n",
        "import math\n",
        "import traceback\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weather Station CSV Maker"
      ],
      "metadata": {
        "id": "VtPVR2OiSLZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeatherStationParser:\n",
        "    def __init__(self, weather_dir):\n",
        "        \"\"\"\n",
        "        weather_dir: Directory containing weather files like \"08022025.txt\"\n",
        "        \"\"\"\n",
        "        self.weather_dir = weather_dir\n",
        "        self.feature_names = [\n",
        "            'UPTIME', 'STATUS', 'EXTDC',\n",
        "            'TAAVG1M', 'TAAVG1H', 'TAAVG1D', 'TAMIN1D', 'TAMAX1D',\n",
        "            'RHAVG1M', 'RHAVG1H', 'RHAVG1D', 'RHMIN1D', 'RHMAX1D',\n",
        "            'TDAVG1M', 'TDAVG1H', 'HTIDXAVG1M',\n",
        "            'PRSUM1M', 'PRSUM1H', 'PRSUM1D',\n",
        "            'WD1', 'WD1AVG2M', 'WD1MIN2M', 'WD1MAX2M',\n",
        "            'WD1AVG10M', 'WD1MIN10M', 'WD1MAX10M',\n",
        "            'WS1', 'WS1AVG2M', 'WS1MIN2M', 'WS1MAX2M',\n",
        "            'WS1AVG10M', 'WS1MIN10M', 'WS1MAX10M',\n",
        "            'WGD1VALUE10M',\n",
        "            'SRAVG1M', 'SRAVG1H', 'SRAVG1D', 'SRMIN1D', 'SRMAX1D',\n",
        "            'SDURSUM1M', 'SDURSUM1D'\n",
        "        ]\n",
        "\n",
        "    def parse_line(self, line, date):\n",
        "        \"\"\"Parse a single weather station line\"\"\"\n",
        "        if not line.startswith('$'):\n",
        "            return None\n",
        "\n",
        "        # Remove $ and split by comma\n",
        "        parts = line[1:].split(',')\n",
        "\n",
        "        # Extract timestamp\n",
        "        timestamp_str = parts[0]\n",
        "        try:\n",
        "            time_parts = timestamp_str.split(':')\n",
        "            hours = int(time_parts[0])\n",
        "            minutes = int(time_parts[1])\n",
        "            seconds = int(time_parts[2])\n",
        "\n",
        "            # Combine date and time\n",
        "            full_datetime = datetime(date.year, date.month, date.day,\n",
        "                                    hours, minutes, seconds)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "        # Parse key-value pairs\n",
        "        data = {'datetime': full_datetime}\n",
        "\n",
        "        i = 1\n",
        "        while i < len(parts):\n",
        "            key = parts[i].strip()\n",
        "            if i + 1 < len(parts):\n",
        "                value_str = parts[i + 1].split('*')[0].strip()\n",
        "                try:\n",
        "                    data[key] = float(value_str)\n",
        "                except ValueError:\n",
        "                    data[key] = value_str\n",
        "\n",
        "                i += 2\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "        return data\n",
        "\n",
        "    def parse_file(self, filepath):\n",
        "        \"\"\"Parse an entire weather file\"\"\"\n",
        "        # Extract date from filename\n",
        "        filename = os.path.basename(filepath)\n",
        "        date_str = filename.replace('.txt', '')\n",
        "\n",
        "        try:\n",
        "            # Parse as DDMMYYYY\n",
        "            day = int(date_str[:2])\n",
        "            month = int(date_str[2:4])\n",
        "            year = int(date_str[4:])\n",
        "            file_date = datetime(year, month, day)\n",
        "        except:\n",
        "            print(f\"Could not parse date from filename: {filename}\")\n",
        "            return None\n",
        "\n",
        "        records = []\n",
        "        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    parsed = self.parse_line(line, file_date)\n",
        "                    if parsed:\n",
        "                        records.append(parsed)\n",
        "\n",
        "        return pd.DataFrame(records)\n",
        "\n",
        "    def parse_all_files(self):\n",
        "        \"\"\"Parse all weather files in the directory\"\"\"\n",
        "        all_data = []\n",
        "\n",
        "        # Get all .txt files\n",
        "        files = sorted([f for f in os.listdir(self.weather_dir) if f.endswith('.txt')])\n",
        "\n",
        "        print(f\"Found {len(files)} weather files\")\n",
        "\n",
        "        for filename in files:\n",
        "            filepath = os.path.join(self.weather_dir, filename)\n",
        "            print(f\"Parsing {filename}...\", end=' ')\n",
        "\n",
        "            df = self.parse_file(filepath)\n",
        "            if df is not None and len(df) > 0:\n",
        "                all_data.append(df)\n",
        "                print(f\"✓ ({len(df)} records)\")\n",
        "            else:\n",
        "                print(\"✗ (failed)\")\n",
        "\n",
        "        if not all_data:\n",
        "            print(\"No data parsed!\")\n",
        "            return None\n",
        "\n",
        "        # Combine all dataframes\n",
        "        combined_df = pd.concat(all_data, ignore_index=True)\n",
        "        combined_df = combined_df.sort_values('datetime').reset_index(drop=True)\n",
        "\n",
        "        print(f\"\\nTotal records: {len(combined_df)}\")\n",
        "        print(f\"Date range: {combined_df['datetime'].min()} to {combined_df['datetime'].max()}\")\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "    def get_weather_at_time(self, target_datetime, df, method='nearest'):\n",
        "        \"\"\"\n",
        "        Get weather data for a specific datetime\n",
        "\n",
        "        method: 'nearest', 'interpolate', or 'forward_fill'\n",
        "        \"\"\"\n",
        "        if df is None or len(df) == 0:\n",
        "            return None\n",
        "\n",
        "        if method == 'nearest':\n",
        "            # Find closest timestamp\n",
        "            time_diffs = abs(df['datetime'] - target_datetime)\n",
        "            closest_idx = time_diffs.argmin()\n",
        "\n",
        "            # Only return if within 10 minutes\n",
        "            if time_diffs.iloc[closest_idx] <= timedelta(minutes=5):\n",
        "                return df.iloc[closest_idx].to_dict()\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "        elif method == 'interpolate':\n",
        "            # Linear interpolation between surrounding points\n",
        "            before = df[df['datetime'] <= target_datetime]\n",
        "            after = df[df['datetime'] >= target_datetime]\n",
        "\n",
        "            if len(before) == 0 or len(after) == 0:\n",
        "                return None\n",
        "\n",
        "            before_record = before.iloc[-1]\n",
        "            after_record = after.iloc[0]\n",
        "\n",
        "            # Calculate interpolation weight\n",
        "            total_time = (after_record['datetime'] - before_record['datetime']).total_seconds()\n",
        "            if total_time == 0:\n",
        "                return before_record.to_dict()\n",
        "\n",
        "            time_from_before = (target_datetime - before_record['datetime']).total_seconds()\n",
        "            weight = time_from_before / total_time\n",
        "\n",
        "            # Interpolate numerical features\n",
        "            result = {'datetime': target_datetime}\n",
        "            for col in df.columns:\n",
        "                if col == 'datetime':\n",
        "                    continue\n",
        "                if pd.api.types.is_numeric_dtype(df[col]):\n",
        "                    result[col] = before_record[col] * (1 - weight) + after_record[col] * weight\n",
        "                else:\n",
        "                    result[col] = before_record[col]\n",
        "\n",
        "            return result\n",
        "\n",
        "        elif method == 'forward_fill':\n",
        "            # Use most recent measurement before target time\n",
        "            before = df[df['datetime'] <= target_datetime]\n",
        "            if len(before) > 0:\n",
        "                return before.iloc[-1].to_dict()\n",
        "            return None\n",
        "\n",
        "    def save_processed_data(self, df, output_path):\n",
        "        \"\"\"Save processed weather data\"\"\"\n",
        "        df.to_csv(output_path, index=False)\n",
        "        print(f\"Saved processed weather data to {output_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    root = os.getcwd()\n",
        "    weather_dir = os.path.join(root, 'LiDAR/weather_station_data')\n",
        "    parser = WeatherStationParser(weather_dir)\n",
        "\n",
        "    # Parse all files\n",
        "    weather_df = parser.parse_all_files()\n",
        "\n",
        "    # Save combined data\n",
        "    if weather_df is not None:\n",
        "        parser.save_processed_data(weather_df, \"weather_data_combined.csv\")\n",
        "\n",
        "        # Example\n",
        "        target_time = datetime(2025, 8, 2, 10, 30)\n",
        "        weather_at_time = parser.get_weather_at_time(target_time, weather_df, method='interpolate')\n",
        "\n",
        "        if weather_at_time:\n",
        "            print(f\"\\nWeather at {target_time}:\")\n",
        "            print(f\"  Temperature: {weather_at_time.get('TAAVG1M', 'N/A')}°C\")\n",
        "            print(f\"  Humidity: {weather_at_time.get('RHAVG1M', 'N/A')}%\")\n",
        "            print(f\"  Wind Speed: {weather_at_time.get('WS1AVG10M', 'N/A')} m/s\")\n",
        "            print(f\"  Wind Direction: {weather_at_time.get('WD1AVG10M', 'N/A')}°\")"
      ],
      "metadata": {
        "id": "4aqCuxW2R-6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-NN Saver"
      ],
      "metadata": {
        "id": "t21bOikfSbUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "DATASET_PATH = \"2025-08-20_11_40.npy\"\n",
        "K_NEIGHBORS = 16\n",
        "\n",
        "OUTPUT_DIR = [\"precomputed_neighbors/K32.pt\"]\n",
        "\n",
        "def precompute_static_indices(x, k=16):\n",
        "    \"\"\"\n",
        "    Pre-computes k-NN indices for a single point cloud.\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): The input point cloud data of shape [B, T, N, F].\n",
        "        k (int): The number of nearest neighbors to find.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The pre-computed indices of shape [B, N, k].\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "      B, T, N, F = x.shape\n",
        "      print(f\"Pre-computing static neighbor indices from an input of shape {x.shape}...\")\n",
        "\n",
        "      # Get xyz for the first (and only) time step\n",
        "      xyz = x[:, 0, :, :3]\n",
        "\n",
        "      if N == 0:\n",
        "          indices = torch.empty(B, 0, k, dtype=torch.long, device=x.device)\n",
        "      else:\n",
        "          k_eff = min(k, N)\n",
        "          d = torch.cdist(xyz, xyz)\n",
        "          eye = torch.eye(N, device=x.device, dtype=d.dtype).unsqueeze(0)\n",
        "          d = d + eye * 1e6\n",
        "          indices = d.topk(k=k_eff, dim=-1, largest=False).indices\n",
        "\n",
        "      print(f\"Completed pre-computation. The resulting tensor shape is: {indices.shape}.\")\n",
        "    return indices\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Main execution logic ---\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    root = os.getcwd()\n",
        "    lidar_dir = os.path.join(root, 'LiDAR')\n",
        "    path = os.path.join(lidar_dir, DATASET_PATH)\n",
        "    if os.path.exists(path):\n",
        "        print(f\"Loading dataset from {path}...\")\n",
        "        try:\n",
        "            # Load the numpy file\n",
        "            numpy_data = np.load(path)\n",
        "            radial_distance = np.sqrt(numpy_data[:, 0]**2 + numpy_data[:, 1]**2 + numpy_data[:, 2]**2)\n",
        "            numpy_data = numpy_data[radial_distance <= 1.1]\n",
        "            # Convert to PyTorch tensor and add batch (B) and time (T) dimensions\n",
        "            full_dataset = torch.from_numpy(numpy_data).float().unsqueeze(0).unsqueeze(0)\n",
        "            print(f\"Loaded and reshaped dataset to a PyTorch tensor of shape: {full_dataset.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading dataset: {e}. Exiting.\")\n",
        "            sys.exit(1)\n",
        "    else:\n",
        "        print(f\"NumPy dataset not found at {path}. Creating a dummy file...\")\n",
        "\n",
        "    for i in range(0, 1):\n",
        "        # Pre-compute the indices\n",
        "        precomputed_indices_tensor = precompute_static_indices(full_dataset, k=(0))\n",
        "        precomputed_indices_tensor = precomputed_indices_tensor.to(dtype=torch.long)\n",
        "        output = os.path.join(lidar_dir, OUTPUT_DIR[i])\n",
        "\n",
        "        # Save the tensor to disk\n",
        "        print(f\"\\nSaving pre-computed indices to: {output}\")\n",
        "        try:\n",
        "            torch.save(precomputed_indices_tensor, output)\n",
        "            print(\"Successfully saved the pre-computed indices. You can now use them in your training script.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving file: {e}\")\n",
        "\n",
        "        torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "omNQQViDSaPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weather Dataset"
      ],
      "metadata": {
        "id": "c08jWq3ZSb0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeatherEnhancedLTSMDataset(LTSMDataset):\n",
        "    def __init__(self, root_dir, seq_list, weather_csv_path, T=6, use_polar=False):\n",
        "        super().__init__(root_dir, seq_list, T, feature_stats=None, use_polar=use_polar)\n",
        "\n",
        "        # Weather features to use\n",
        "        self.weather_features = [\n",
        "            'TAAVG1M',      # Temperature\n",
        "            'RHAVG1M',      # Humidity\n",
        "            'TDAVG1M',      # Dew point\n",
        "            'WS1AVG10M',    # Wind speed\n",
        "            'WD1AVG10M',    # Wind direction\n",
        "            'PRSUM1H',      # Precipitation\n",
        "        ]\n",
        "\n",
        "        # Load only required columns\n",
        "        columns_to_load = ['datetime'] + self.weather_features\n",
        "        self.weather_df = pd.read_csv(\n",
        "            weather_csv_path,\n",
        "            usecols=columns_to_load,\n",
        "            low_memory=False\n",
        "        )\n",
        "        self.weather_df['datetime'] = pd.to_datetime(self.weather_df['datetime'])\n",
        "\n",
        "        # Normalization ranges for weather\n",
        "        self.weather_norm = {\n",
        "            'TAAVG1M': (-10.0, 40.0),\n",
        "            'RHAVG1M': (0.0, 100.0),\n",
        "            'TDAVG1M': (-20.0, 30.0),\n",
        "            'WS1AVG10M': (0.0, 30.0),\n",
        "            'WD1AVG10M': (0.0, 360.0),\n",
        "            'PRSUM1H': (0.0, 50.0),\n",
        "        }\n",
        "\n",
        "        if use_polar:\n",
        "            self.grad_dir = os.path.join(root_dir, 'gradients_polar')\n",
        "        else:\n",
        "            self.grad_dir = os.path.join(root_dir, 'gradients')\n",
        "\n",
        "        # Check if gradients exist\n",
        "        self.has_precomputed_gradients = os.path.exists(self.grad_dir)\n",
        "\n",
        "        if not self.has_precomputed_gradients:\n",
        "            print(\"WARNING: No precomputed gradients found. Will compute on-the-fly (slow!)\")\n",
        "            print(f\"Run precompute_gradients.py to create {self.grad_dir}\")\n",
        "\n",
        "    def verify_geometric_consistency(self):\n",
        "        \"\"\"Check if all point clouds have consistent geometry\"\"\"\n",
        "        sample_scans = []\n",
        "        for i in range(min(10, len(self.seq_list))):\n",
        "            pc = self.load_point_cloud(self.seq_list[i][0])\n",
        "            if pc is not None:\n",
        "                # Get azimuth, range, elevation\n",
        "                coords = pc[:, :3]\n",
        "                sample_scans.append(coords)\n",
        "\n",
        "        # Check if all scans have same shape and similar coordinate distributions\n",
        "        for i, scan in enumerate(sample_scans):\n",
        "            print(f\"Scan {i}: shape={scan.shape}, \"\n",
        "                  f\"az range=[{scan[:,0].min():.1f}, {scan[:,0].max():.1f}], \"\n",
        "                  f\"range range=[{scan[:,1].min():.1f}, {scan[:,1].max():.1f}]\")\n",
        "\n",
        "    def get_weather_at_time(self, target_datetime, max_time_gap_minutes=15):\n",
        "        \"\"\"Get interpolated weather data for a specific datetime\"\"\"\n",
        "        before = self.weather_df[self.weather_df['datetime'] <= target_datetime]\n",
        "        after = self.weather_df[self.weather_df['datetime'] >= target_datetime]\n",
        "\n",
        "        if len(before) == 0 or len(after) == 0:\n",
        "            # Check if nearest data is within acceptable range\n",
        "            time_diffs = abs(self.weather_df['datetime'] - target_datetime)\n",
        "            min_diff = time_diffs.min()\n",
        "\n",
        "            # If closest weather data is more than max_time_gap away, return None\n",
        "            if min_diff > pd.Timedelta(minutes=max_time_gap_minutes):\n",
        "                return None\n",
        "\n",
        "            closest_idx = time_diffs.argmin()\n",
        "            return self.weather_df.iloc[closest_idx]\n",
        "\n",
        "        before_record = before.iloc[-1]\n",
        "        after_record = after.iloc[0]\n",
        "\n",
        "        # Check if the gap is too large\n",
        "        before_gap = abs((target_datetime - before_record['datetime']).total_seconds() / 60)\n",
        "        after_gap = abs((after_record['datetime'] - target_datetime).total_seconds() / 60)\n",
        "\n",
        "        if before_gap > max_time_gap_minutes and after_gap > max_time_gap_minutes:\n",
        "            return None\n",
        "\n",
        "        if before_record['datetime'] == target_datetime:\n",
        "            return before_record\n",
        "\n",
        "        # Linear interpolation\n",
        "        total_seconds = (after_record['datetime'] - before_record['datetime']).total_seconds()\n",
        "        if total_seconds == 0:\n",
        "            return before_record\n",
        "\n",
        "        elapsed_seconds = (target_datetime - before_record['datetime']).total_seconds()\n",
        "        weight = elapsed_seconds / total_seconds\n",
        "\n",
        "        result = {}\n",
        "        for feature in self.weather_features:\n",
        "            before_val = before_record.get(feature, None)\n",
        "            after_val = after_record.get(feature, None)\n",
        "\n",
        "            if pd.isna(before_val) or pd.isna(after_val) or before_val is None or after_val is None:\n",
        "                # If any required feature is missing, return None for the whole record\n",
        "                return None\n",
        "\n",
        "            if feature == 'WD1AVG10M':\n",
        "                # Circular interpolation for wind direction\n",
        "                diff = (after_val - before_val + 180) % 360 - 180\n",
        "                result[feature] = (before_val + diff * weight) % 360\n",
        "            else:\n",
        "                result[feature] = before_val * (1 - weight) + after_val * weight\n",
        "\n",
        "        return result\n",
        "\n",
        "    def normalize_weather(self, feature_name, value):\n",
        "        \"\"\"Normalize weather feature to [0, 1]\"\"\"\n",
        "        if feature_name not in self.weather_norm:\n",
        "            return value\n",
        "\n",
        "        min_val, max_val = self.weather_norm[feature_name]\n",
        "        value = np.clip(value, min_val, max_val)\n",
        "        return (value - min_val) / (max_val - min_val)\n",
        "\n",
        "    def get_weather_features_normalized(self, target_datetime):\n",
        "        \"\"\"Get normalized weather features as numpy array, returns None if unavailable\"\"\"\n",
        "        weather_data = self.get_weather_at_time(target_datetime)\n",
        "\n",
        "        # Return None if no weather data available\n",
        "        if weather_data is None:\n",
        "            return None\n",
        "\n",
        "        features = []\n",
        "        for feature_name in self.weather_features:\n",
        "            value = weather_data.get(feature_name, None)\n",
        "\n",
        "            # If any feature is missing, return None\n",
        "            if value is None or pd.isna(value):\n",
        "                return None\n",
        "\n",
        "            normalized = self.normalize_weather(feature_name, value)\n",
        "            features.append(normalized)\n",
        "\n",
        "        return np.array(features, dtype=np.float32)\n",
        "\n",
        "    def compute_upwind_cnr_polar(self, pc_tensor, precomputed_indices, dt=600):\n",
        "        \"\"\"\n",
        "        Compute upwind CNR using pre-computed neighbor indices\n",
        "\n",
        "        pc_tensor: [N, 7] - [azimuth_deg, range_m, elevation_deg, cnr, wind_r, wind_t, wind_v]\n",
        "        precomputed_indices: [N, k] - pre-computed k nearest neighbors for each point\n",
        "        \"\"\"\n",
        "        N = pc_tensor.shape[0]\n",
        "        k = precomputed_indices.shape[1]\n",
        "\n",
        "        # Ensure indices are on the same device as pc_tensor\n",
        "        precomputed_indices = precomputed_indices.to(pc_tensor.device)\n",
        "\n",
        "        azimuth = pc_tensor[:, 0]\n",
        "        range_m = pc_tensor[:, 1]\n",
        "        elevation = pc_tensor[:, 2]\n",
        "        current_cnr = pc_tensor[:, 3]\n",
        "\n",
        "        wind_radial = pc_tensor[:, 4]\n",
        "        wind_tangential = pc_tensor[:, 5]\n",
        "        wind_vertical = pc_tensor[:, 6]\n",
        "\n",
        "        # Convert to radians\n",
        "        az_rad = torch.deg2rad(azimuth)\n",
        "        el_rad = torch.deg2rad(elevation)\n",
        "\n",
        "        # Polar to Cartesian\n",
        "        x = range_m * torch.cos(el_rad) * torch.cos(az_rad)\n",
        "        y = range_m * torch.cos(el_rad) * torch.sin(az_rad)\n",
        "        z = range_m * torch.sin(el_rad)\n",
        "\n",
        "        # Wind displacement components\n",
        "        dx_radial = wind_radial * torch.cos(el_rad) * torch.cos(az_rad)\n",
        "        dy_radial = wind_radial * torch.cos(el_rad) * torch.sin(az_rad)\n",
        "        dz_radial = wind_radial * torch.sin(el_rad)\n",
        "\n",
        "        dx_tangential = -wind_tangential * torch.sin(az_rad)\n",
        "        dy_tangential = wind_tangential * torch.cos(az_rad)\n",
        "\n",
        "        # Source positions (where air came from)\n",
        "        source_x = x - (dx_radial + dx_tangential) * dt\n",
        "        source_y = y - (dy_radial + dy_tangential) * dt\n",
        "        source_z = z - (dz_radial + wind_vertical) * dt\n",
        "\n",
        "        current_positions = torch.stack([x, y, z], dim=1)\n",
        "        source_positions = torch.stack([source_x, source_y, source_z], dim=1)\n",
        "\n",
        "        # Gather neighbor\n",
        "        neighbor_positions = current_positions[precomputed_indices]\n",
        "\n",
        "        # Compute distances from source\n",
        "        source_expanded = source_positions.unsqueeze(1)\n",
        "        distances = torch.norm(neighbor_positions - source_expanded, dim=2)\n",
        "\n",
        "        # Find closest among pre-computed neighbors\n",
        "        closest_idx = distances.argmin(dim=1)\n",
        "\n",
        "        # Map back to actual point indices\n",
        "        point_idx = torch.arange(N, device=pc_tensor.device)\n",
        "        upwind_point_idx = precomputed_indices[point_idx, closest_idx]\n",
        "\n",
        "        # Get upwind CNR\n",
        "        upwind_cnr = current_cnr[upwind_point_idx]\n",
        "        advection_delta = upwind_cnr - current_cnr\n",
        "\n",
        "        return upwind_cnr, advection_delta\n",
        "\n",
        "    def compute_radial_gradient_polar(self, pc_tensor):\n",
        "        \"\"\"\n",
        "        Compute absolute CNR change along radial direction\n",
        "\n",
        "        Returns:\n",
        "            radial_grad: [N] - Absolute CNR change (range: [-1, 1])\n",
        "            beam_cnr_std: [N] - CNR std within beam\n",
        "            boundary_proximity: [N] - Normalized distance to strong gradients\n",
        "        \"\"\"\n",
        "        N = pc_tensor.shape[0]\n",
        "        device = pc_tensor.device\n",
        "\n",
        "        # Extract features\n",
        "        azimuth, range_m, elevation, cnr = pc_tensor[:, 0], pc_tensor[:, 1], pc_tensor[:, 2], pc_tensor[:, 3]\n",
        "\n",
        "        # Create beam IDs\n",
        "        az_bins = torch.round(azimuth).long()\n",
        "        el_bins = torch.round(elevation).long()\n",
        "        beam_id = (az_bins + 360) * 1000 + (el_bins + 90)\n",
        "\n",
        "        # Initialize outputs\n",
        "        radial_grad = torch.zeros(N, device=device)\n",
        "        beam_cnr_std = torch.zeros(N, device=device)\n",
        "        boundary_proximity = torch.ones(N, device=device) * 10.0\n",
        "\n",
        "        # Process each beam\n",
        "        for beam in torch.unique(beam_id):\n",
        "            mask = beam_id == beam\n",
        "            indices = torch.where(mask)[0]\n",
        "\n",
        "            if len(indices) < 2:\n",
        "                continue\n",
        "\n",
        "            # Sort by range\n",
        "            beam_ranges = range_m[indices]\n",
        "            beam_cnrs = cnr[indices]\n",
        "            sorted_idx = torch.argsort(beam_ranges)\n",
        "\n",
        "            # Sorted data\n",
        "            sorted_cnrs = beam_cnrs[sorted_idx]\n",
        "            sorted_ranges = beam_ranges[sorted_idx]\n",
        "            orig_indices = indices[sorted_idx]\n",
        "\n",
        "            cnr_changes = torch.diff(sorted_cnrs)\n",
        "\n",
        "            radial_grad[orig_indices[:-1]] = cnr_changes\n",
        "            radial_grad[orig_indices[-1]] = cnr_changes[-1] if len(cnr_changes) > 0 else 0.0\n",
        "\n",
        "            beam_cnr_std[indices] = torch.std(beam_cnrs) if len(beam_cnrs) > 1 else 0.0\n",
        "\n",
        "            strong_boundaries = torch.abs(cnr_changes) > 0.3\n",
        "            if strong_boundaries.any():\n",
        "                boundary_locs = sorted_ranges[:-1][strong_boundaries]\n",
        "                median_spacing = torch.median(torch.diff(sorted_ranges))\n",
        "\n",
        "                for i, idx in enumerate(orig_indices):\n",
        "                    if len(boundary_locs) > 0:\n",
        "                        dist = torch.min(torch.abs(boundary_locs - sorted_ranges[i]))\n",
        "                        boundary_proximity[idx] = torch.clamp(dist / (median_spacing + 1e-6), 0.0, 10.0)\n",
        "\n",
        "        return radial_grad.unsqueeze(1), beam_cnr_std.unsqueeze(1), boundary_proximity.unsqueeze(1)\n",
        "\n",
        "    def load_radial_gradient(self, dt):\n",
        "        \"\"\"Load precomputed radial gradient for a timestamp\"\"\"\n",
        "        if not self.has_precomputed_gradients:\n",
        "            return None\n",
        "\n",
        "        if self.use_polar:\n",
        "            fname = f\"{dt.strftime('%Y-%m-%d')}_{dt.hour}_{dt.strftime('%M')}_polar.npy\"\n",
        "        else:\n",
        "            fname = f\"{dt.strftime('%Y-%m-%d')}_{dt.hour}_{dt.strftime('%M')}.npy\"\n",
        "\n",
        "        grad_path = os.path.join(self.grad_dir, fname)\n",
        "\n",
        "        if not os.path.exists(grad_path):\n",
        "            print(f\"Missing gradient file: {grad_path}\")\n",
        "            return None\n",
        "\n",
        "        return np.load(grad_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        times = self.seq_list[idx]\n",
        "        pcs = []\n",
        "\n",
        "        # Load T+1 frames\n",
        "        for i in range(self.T + 1):\n",
        "            pc = self.load_point_cloud(times[i])\n",
        "            if pc is None:\n",
        "                return None\n",
        "            pcs.append(pc)\n",
        "\n",
        "        # Uniform size\n",
        "        max_points = max(pc.shape[0] for pc in pcs)\n",
        "        pcs_tensor = []\n",
        "\n",
        "        for i, pc in enumerate(pcs):\n",
        "            pc_tensor = self.downsample_point_cloud(pc, max_points)\n",
        "            pc_tensor = self.standardize_features(pc_tensor)\n",
        "\n",
        "            # Get indices\n",
        "            if hasattr(self, 'precomputed_indices') and max_points in self.precomputed_indices:\n",
        "                indices = self.precomputed_indices[max_points]\n",
        "            else:\n",
        "                indices = precomputed_indices_tensor[0, :max_points, :]\n",
        "\n",
        "            # Compute upwind features\n",
        "            if self.use_polar:\n",
        "                upwind_cnr, advection_delta = self.compute_upwind_cnr_polar(\n",
        "                    pc_tensor, indices\n",
        "                )\n",
        "                upwind_cnr_expanded = upwind_cnr.unsqueeze(1)\n",
        "                advection_delta_expanded = advection_delta.unsqueeze(1)\n",
        "                radial_grad_np = self.load_radial_gradient(times[i])\n",
        "\n",
        "                if radial_grad_np is not None:\n",
        "                    # Downsample/pad gradient to match point cloud\n",
        "                    N_orig = radial_grad_np.shape[0]\n",
        "                    if N_orig < max_points:\n",
        "                        # Pad with zeros\n",
        "                        pad = np.zeros(max_points - N_orig, dtype=np.float32)\n",
        "                        radial_grad_np = np.concatenate([radial_grad_np, pad])\n",
        "                    elif N_orig > max_points:\n",
        "                        radial_grad_np = radial_grad_np[:max_points]\n",
        "\n",
        "                    radial_grad = torch.from_numpy(radial_grad_np).float().unsqueeze(1)\n",
        "                else:\n",
        "                    # Fallback: compute on-the-fly\n",
        "                    radial_grad, _, _ = self.compute_radial_gradient_polar(pc_tensor)\n",
        "            else:\n",
        "                N = pc_tensor.shape[0]\n",
        "                upwind_cnr_expanded = torch.zeros(N, 1, device=pc_tensor.device)\n",
        "                advection_delta_expanded = torch.zeros(N, 1, device=pc_tensor.device)\n",
        "                radial_grad = torch.zeros(N, 1, device=pc_tensor.device)\n",
        "\n",
        "            # Concatenate features\n",
        "            enhanced_pc = torch.cat([\n",
        "                pc_tensor,\n",
        "                upwind_cnr_expanded,\n",
        "                advection_delta_expanded\n",
        "            ], dim=1)\n",
        "\n",
        "            pcs_tensor.append(enhanced_pc)\n",
        "\n",
        "        return {f'pc{i}': pc for i, pc in enumerate(pcs_tensor)}\n"
      ],
      "metadata": {
        "id": "A4hXAtkxSaaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Dataset Loading"
      ],
      "metadata": {
        "id": "Q7qdM1SEScLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max_scale(tensor, min_val, max_val):\n",
        "    if tensor.numel() == 0:\n",
        "        return tensor\n",
        "\n",
        "    # Avoid division by zero if min_val and max_val are equal\n",
        "    if max_val == min_val:\n",
        "        return torch.zeros_like(tensor)\n",
        "    else:\n",
        "        scaled_tensor = (tensor - min_val) / (max_val - min_val)\n",
        "        return scaled_tensor\n",
        "\n",
        "def standardize(tensor, mean, std):\n",
        "    if tensor.numel() == 0:\n",
        "        return tensor\n",
        "\n",
        "    # Avoid division by zero if std is zero\n",
        "    if std == 0:\n",
        "        return torch.zeros_like(tensor)\n",
        "    else:\n",
        "        standardized_tensor = (tensor - mean) / std\n",
        "        return standardized_tensor\n",
        "\n",
        "def log_normalize_cnr(cnr_tensor, cnr_shift, cnr_log_mean, cnr_log_std):\n",
        "    \"\"\"Apply log transformation and normalization to CNR values\"\"\"\n",
        "    if cnr_tensor.numel() == 0:\n",
        "        return cnr_tensor\n",
        "\n",
        "    # Shift CNR to positive range\n",
        "    cnr_shifted = cnr_tensor + cnr_shift\n",
        "    # Safety clamp to avoid log(0) or log(negative)\n",
        "    cnr_shifted = torch.clamp(cnr_shifted, min=1e-6)\n",
        "    # Log transform\n",
        "    cnr_log = torch.log(cnr_shifted)\n",
        "    # Normalize using global log statistics\n",
        "    cnr_normalized = (cnr_log - cnr_log_mean) / cnr_log_std\n",
        "\n",
        "    return cnr_normalized\n",
        "\n",
        "def normalize_winds_with_nonzero_stats(wind_values, scale):\n",
        "    threshold = 1e-6\n",
        "    zero_mask = torch.abs(wind_values) < threshold\n",
        "\n",
        "    # Use p95 as scale (or std, your choice)\n",
        "    normalized = wind_values / scale\n",
        "\n",
        "    # Keep zeros as zeros\n",
        "    normalized[zero_mask] = 0.0\n",
        "\n",
        "    return normalized\n",
        "\n",
        "class LTSMDataset(Dataset):\n",
        "    def __init__(self, root_dir, seq_list, T=6, feature_stats=None, use_polar=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.pc_dir = os.path.join(root_dir, 'diff_clouds_polar' if use_polar else 'diff_clouds')\n",
        "        self.pc_files = natsort.natsorted(os.listdir(self.pc_dir))\n",
        "        self.seq_list = seq_list\n",
        "        self.T = T\n",
        "        self.use_polar = use_polar\n",
        "        self.max_radial_distance = 1.2\n",
        "\n",
        "        # Keep your existing hardcoded stats\n",
        "        self.hardcoded_min_stats = [-0.9845121092520055, -0.9845031637333835, 0.001796004840885169, -100.0, -2.2024194711095904, -1.8266775445970527, -2.3839595067237744, 1753711807.471]\n",
        "        self.hardcoded_max_stats = [0.9845165259173817, 0.9845031637333835, 0.988206682848428, 91.48, 2.2221496199313333, 1.883747076787428, 2.025801534327347, 1754488505.001]\n",
        "        self.hardcoded_mean_stats = [2.7022033600688215e-06, 4.1019552380086045e-06, 0.3304346729575734, -30.466586771026023, -0.012333, -0.005744, -0.002197, 1754092671.0763593]\n",
        "        self.hardcoded_std_stats = [0.291664816512239, 0.2916651588805414, 0.24687948625791928, 4.496726643996928, 0.040166, 0.033383, 0.044368, 220765.19726839956]\n",
        "\n",
        "        self.feature_stats = {\n",
        "            'min': self.hardcoded_min_stats,\n",
        "            'max': self.hardcoded_max_stats,\n",
        "            'mean': self.hardcoded_mean_stats,\n",
        "            'std': self.hardcoded_std_stats,\n",
        "            'wind_scale': [0.15, 0.15, 0.15]\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seq_list)\n",
        "\n",
        "    def load_point_cloud(self, dt):\n",
        "        if self.use_polar:\n",
        "            fname = f\"{dt.strftime('%Y-%m-%d')}_{dt.hour}_{dt.strftime('%M')}_polar.npy\"\n",
        "        else:\n",
        "            fname = f\"{dt.strftime('%Y-%m-%d')}_{dt.hour}_{dt.strftime('%M')}.npy\"\n",
        "\n",
        "        path = os.path.join(self.pc_dir, fname)\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"Missing point cloud: {path}\")\n",
        "            return None\n",
        "\n",
        "        pc = np.load(path)\n",
        "        if np.isnan(pc).any():\n",
        "            print(f'Nan in file: {fname}')\n",
        "            pc = np.nan_to_num(pc)\n",
        "\n",
        "        # Apply radial distance filter\n",
        "        if self.use_polar:\n",
        "            # For polar: range is in column 1\n",
        "            pc = pc[pc[:, 1] <= self.max_radial_distance * 14500]\n",
        "        else:\n",
        "            # For Cartesian: compute radial distance\n",
        "            radial_distance = np.sqrt(pc[:, 0]**2 + pc[:, 1]**2 + pc[:, 2]**2)\n",
        "            pc = pc[radial_distance <= self.max_radial_distance]\n",
        "\n",
        "        return pc\n",
        "\n",
        "    def standardize_features(self, pc_tensor):\n",
        "        if self.use_polar:\n",
        "            # Polar coordinate normalization\n",
        "            # Azimuth: [-180, 180] -> [-1, 1]\n",
        "            pc_tensor[:, 0] = pc_tensor[:, 0] / 180\n",
        "            # Range: [0, 14500] -> [0, 1]\n",
        "            pc_tensor[:, 1] = pc_tensor[:, 1] / 14500\n",
        "            # Elevation: [-90, 90] -> [-1, 1]\n",
        "            pc_tensor[:, 2] = pc_tensor[:, 2] / 90\n",
        "            # CNR: Apply your supervisor's [-20, 20] constraint\n",
        "            cnr_clipped = torch.clamp(pc_tensor[:, 3], -40, 10)\n",
        "            pc_tensor[:, 3] = (cnr_clipped + 40) / 50  # [0, 1]\n",
        "            pc_tensor = pc_tensor[:, :-1]\n",
        "        else:\n",
        "            pc_tensor = pc_tensor[:, :-1]\n",
        "            for i in range(3):\n",
        "                pc_tensor[:, i] = (pc_tensor[:, i] - self.feature_stats['mean'][i]) / self.feature_stats['std'][i]\n",
        "            cnr_clipped = torch.clamp(pc_tensor[:, 3], -40, 10)\n",
        "            pc_tensor[:, 3] = (cnr_clipped + 40) / 50  # [0, 1]\n",
        "\n",
        "        return pc_tensor\n",
        "\n",
        "    def downsample_point_cloud(self, pc, target_points):\n",
        "\n",
        "        N = pc.shape[0]\n",
        "        if N < target_points:\n",
        "            pad = np.zeros((target_points-N, pc.shape[1]), dtype=np.float32)\n",
        "            pc = np.vstack([pc, pad])\n",
        "        elif N > target_points:\n",
        "            idx = np.random.choice(N, target_points, replace=False)\n",
        "            pc = pc[idx]\n",
        "        return torch.from_numpy(pc).float()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        times = self.seq_list[idx]\n",
        "        pcs = []\n",
        "        for i in range(self.T+1):\n",
        "            pc = self.load_point_cloud(times[i])\n",
        "            if pc is None:\n",
        "                return None\n",
        "            pcs.append(pc)\n",
        "\n",
        "        max_points = max(pc.shape[0] for pc in pcs)\n",
        "        pcs_tensor = []\n",
        "        for pc in pcs:\n",
        "            pc_tensor = self.downsample_point_cloud(pc, max_points)\n",
        "            pc_tensor = self.standardize_features(pc_tensor)\n",
        "            pcs_tensor.append(pc_tensor)\n",
        "\n",
        "        return {f'pc{i}': pc for i, pc in enumerate(pcs_tensor)}\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        # Keep your existing logic\n",
        "        batch = [b for b in batch if b is not None]\n",
        "        if not batch: return {}\n",
        "        keys = batch[0].keys()\n",
        "        return {k: torch.stack([b[k] for b in batch], dim=0) for k in keys}\n"
      ],
      "metadata": {
        "id": "XCUM9E-GSadY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Section"
      ],
      "metadata": {
        "id": "QW4JqsPWSciB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gather_neighbors(x, idx):\n",
        "    \"\"\"\n",
        "    x:   [B, N, C]\n",
        "    idx: [B, N, k]\n",
        "    ->   [B, N, k, C]\n",
        "    \"\"\"\n",
        "    B, N, C = x.shape\n",
        "    if N == 0 or idx.shape[2] == 0: # Handle empty point clouds or empty indices\n",
        "        return torch.empty(B, N, idx.shape[2], C, dtype=x.dtype, device=x.device)\n",
        "\n",
        "    idx_flat = idx.view(B, N * idx.shape[-1])\n",
        "    batch_indices = torch.arange(B, device = x.device, dtype=torch.long).unsqueeze(1).repeat(1, N * idx.shape[-1])\n",
        "    gathered_flat = x[batch_indices, idx_flat]\n",
        "\n",
        "    # Reshape the gathered tensor to the final output shape\n",
        "    return gathered_flat.view(B, N, idx.shape[-1], C)\n",
        "\n",
        "class CloudLSTMCell(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, msg_dim, k):\n",
        "        super().__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.msg_dim = msg_dim\n",
        "        self.k = k\n",
        "\n",
        "        self.feat_linear = nn.Linear(in_dim, hidden_dim)\n",
        "\n",
        "        self.msg_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, msg_dim), nn.ReLU(),\n",
        "            nn.Linear(msg_dim, msg_dim), nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTMCell(msg_dim + hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, feat_t, indices, h_t_minus_1=None, c_t_minus_1=None):\n",
        "        B, N, _ = feat_t.shape\n",
        "\n",
        "        if h_t_minus_1 is None:\n",
        "            h_t_minus_1 = torch.zeros(B, N, self.hidden_dim, device=feat_t.device)\n",
        "            c_t_minus_1 = torch.zeros(B, N, self.hidden_dim, device=feat_t.device)\n",
        "\n",
        "        if self.k == 0:\n",
        "            msg = torch.zeros(B, N, self.msg_dim, device=feat_t.device)\n",
        "        else:\n",
        "            # KNN and neighbor gathering\n",
        "            h_neighbors = gather_neighbors(h_t_minus_1, indices)\n",
        "            # Message passing\n",
        "            h_t_minus_1_expanded = h_t_minus_1.unsqueeze(2).repeat(1, 1, self.k, 1)\n",
        "            msg_input = torch.cat([h_t_minus_1_expanded, h_neighbors], dim=-1)\n",
        "            msg = self.msg_mlp(msg_input).sum(dim=2)\n",
        "\n",
        "        # LSTM cell update\n",
        "        lstm_input = torch.cat([msg, self.feat_linear(feat_t)], dim=-1)\n",
        "        h_t, c_t = self.lstm(lstm_input.view(B*N, -1), (h_t_minus_1.view(B*N, -1), c_t_minus_1.view(B*N, -1)))\n",
        "\n",
        "        return h_t.view(B, N, -1), c_t.view(B, N, -1)\n",
        "\n",
        "class CloudLSTMNextScan(nn.Module):\n",
        "    \"\"\"\n",
        "    Input:   sequence of T frames, each [N,8] (xyz + CNR + wind(3) + conf)\n",
        "             shaped as [B,T,N,8]\n",
        "    Output:  predicted next full feature vector [B,N,8] (xyz + CNR + wind(3) + conf at t+1)\n",
        "             where xyz are copied from the last input frame.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=8, hidden_dim=128, msg_dim=64, k=16, T=6):\n",
        "        super().__init__()\n",
        "        self.T = T\n",
        "        self.in_dim = in_dim\n",
        "        self.cell = CloudLSTMCell(in_dim=in_dim, hidden_dim=hidden_dim, msg_dim=msg_dim, k=k)\n",
        "\n",
        "        # Number of features to predict (non-xyz)\n",
        "        self.pred_dim = 1\n",
        "\n",
        "        # The Prediction head\n",
        "        self.mean_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 64), nn.ReLU(),\n",
        "            nn.Linear(64, self.pred_dim), # Outputs the predicted mean (delta features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, precomputed_indices):  # x: [B,T,N,F]\n",
        "        B, T, N, F = x.shape\n",
        "        assert T >= 2, \"Need at least two frames (t-1, t).\"\n",
        "\n",
        "        # Handle empty point clouds in the batch\n",
        "        if N == 0:\n",
        "            return (torch.empty(B, N, self.in_dim, device=x.device, dtype=x.dtype),\n",
        "                    torch.empty(B, N, self.pred_dim, device=x.device, dtype=x.dtype),\n",
        "                    torch.empty(B, N, self.pred_dim, device=x.device, dtype=x.dtype))\n",
        "\n",
        "        h = c = None\n",
        "        # Unroll over time (uses per-frame xyz for neighborhoods)\n",
        "        for t in range(T):\n",
        "            feat_t = x[:, t, :, :]\n",
        "            h, c = self.cell(feat_t, precomputed_indices, h, c)\n",
        "\n",
        "        # Predict the delta CNR\n",
        "        pred_delta_features = self.mean_head(h)  # [B,N,1]\n",
        "\n",
        "        # Get the last full feature vector (xyz + additional features)\n",
        "        last_features = x[:, -1, :, :7]\n",
        "\n",
        "        # Get the constant coordinates from the last frame\n",
        "        last_xyz = last_features[:, :, :3] # [B,N,3]\n",
        "\n",
        "        # Get the additional features from the last frame\n",
        "        last_additional_features = last_features[:, :, 3:4]\n",
        "        last_wind_features = last_features[:, :, 4:7]\n",
        "\n",
        "        # Predict the next additional feature vector by adding the delta\n",
        "        pred_next_additional_feat = last_additional_features + pred_delta_features\n",
        "\n",
        "        # Concatenate the constant's with the new predicted CNR\n",
        "        pred_next_feat = torch.cat([last_xyz, pred_next_additional_feat, last_wind_features], dim=-1)\n",
        "\n",
        "        return pred_next_feat, pred_delta_features\n"
      ],
      "metadata": {
        "id": "6v5Y1LenSagg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Losses"
      ],
      "metadata": {
        "id": "cIef5JG-SdBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss with no additional weightings\n",
        "def adaptive_loss(pred_cnr, tgt_cnr, prev_cnr):\n",
        "    \"\"\"\n",
        "    Loss that focuses on dynamic regions while maintaining overall accuracy\n",
        "    \"\"\"\n",
        "    # 1. Base MSE for overall accuracy\n",
        "    loss_mse = F.mse_loss(pred_cnr, tgt_cnr)\n",
        "\n",
        "    # 2. Delta MSE for temporal consistency\n",
        "    tgt_delta = tgt_cnr - prev_cnr\n",
        "    pred_delta = pred_cnr - prev_cnr\n",
        "    loss_delta = F.mse_loss(pred_delta, tgt_delta)\n",
        "\n",
        "    # 3. Dynamic region emphasis (|Δ| > 0.05)\n",
        "    dynamic_mask = torch.abs(tgt_delta) > 0.05\n",
        "    if dynamic_mask.any():\n",
        "        loss_dynamic = F.mse_loss(pred_cnr[dynamic_mask], tgt_cnr[dynamic_mask])\n",
        "    else:\n",
        "        loss_dynamic = torch.tensor(0.0, device=pred_cnr.device)\n",
        "\n",
        "    # 4. Variance preservation\n",
        "    loss_var = torch.abs(pred_cnr.std() - tgt_cnr.std())\n",
        "\n",
        "    loss = 0.3 * loss_mse + 0.4 * loss_dynamic + 0.2 * loss_delta + 0.1 * loss_var\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Loss with adaptive MSE weighting\n",
        "def adaptive_loss_with_cnr_weighting(pred_cnr, tgt_cnr, prev_cnr,\n",
        "                                      cnr_weight_power=2.0,\n",
        "                                      cnr_weight_scale=3.0):\n",
        "    \"\"\"\n",
        "    Adaptive loss with exponential weighting for high CNR values\n",
        "\n",
        "    Args:\n",
        "        cnr_weight_power: How aggressively to weight high CNR\n",
        "        cnr_weight_scale: Maximum weight multiplier for highest CNR\n",
        "    \"\"\"\n",
        "    # 1. Base MSE for overall accuracy\n",
        "    loss_mse = F.mse_loss(pred_cnr, tgt_cnr)\n",
        "\n",
        "    # 2. Delta MSE for temporal consistency\n",
        "    tgt_delta = tgt_cnr - prev_cnr\n",
        "    pred_delta = pred_cnr - prev_cnr\n",
        "    loss_delta = F.mse_loss(pred_delta, tgt_delta)\n",
        "\n",
        "    # 3. Dynamic region emphasis (|Δ| > 0.05)\n",
        "    dynamic_mask = torch.abs(tgt_delta) > 0.05\n",
        "    if dynamic_mask.any():\n",
        "        loss_dynamic = F.mse_loss(pred_cnr[dynamic_mask], tgt_cnr[dynamic_mask])\n",
        "    else:\n",
        "        loss_dynamic = torch.tensor(0.0, device=pred_cnr.device)\n",
        "\n",
        "    # 4. Variance preservation\n",
        "    loss_var = torch.abs(pred_cnr.std() - tgt_cnr.std())\n",
        "\n",
        "    # 5. Exponential weighting\n",
        "    cnr_weights = 1.0 + (cnr_weight_scale - 1.0) * torch.pow(tgt_cnr, cnr_weight_power)\n",
        "    weighted_error = cnr_weights * (pred_cnr - tgt_cnr) ** 2\n",
        "    loss_cnr_weighted = weighted_error.mean()\n",
        "\n",
        "    # Combined loss\n",
        "    loss = (0.2 * loss_mse +\n",
        "            0.3 * loss_dynamic +\n",
        "            0.1 * loss_delta +\n",
        "            0.05 * loss_var +\n",
        "            0.35 * loss_cnr_weighted)\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Loss with adaptive and changing weighted Huber\n",
        "class ImprovedCloudDynamicsLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Combines best of Experiment 171 with improved weighting\n",
        "    \"\"\"\n",
        "    def __init__(self, max_epochs=5, huber_delta=0.2,\n",
        "                 cloud_boost=1.5, dynamic_threshold=0.05):\n",
        "        super().__init__()\n",
        "        self.max_epochs = max_epochs\n",
        "        self.current_epoch = 0\n",
        "        self.huber_delta = huber_delta\n",
        "        self.cloud_boost = cloud_boost\n",
        "        self.dynamic_threshold = dynamic_threshold\n",
        "\n",
        "        # Huber loss for robustness\n",
        "        self.huber = nn.HuberLoss(reduction='none', delta=huber_delta)\n",
        "\n",
        "    def forward(self, pred_cnr, tgt_cnr, prev_cnr, radial_grad=None):\n",
        "        \"\"\"\n",
        "        Compute multi-component cloud-aware loss\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute deltas\n",
        "        pred_delta = pred_cnr - prev_cnr\n",
        "        tgt_delta = tgt_cnr - prev_cnr\n",
        "\n",
        "        # 1: Base MSE (global accuracy)\n",
        "        loss_mse = F.mse_loss(pred_cnr, tgt_cnr)\n",
        "\n",
        "        # 2: Dynamic Region Emphasis (|Δ| > threshold)\n",
        "        dynamic_mask = torch.abs(tgt_delta) > self.dynamic_threshold\n",
        "\n",
        "        if dynamic_mask.any():\n",
        "            loss_dynamic = F.mse_loss(\n",
        "                pred_cnr[dynamic_mask],\n",
        "                tgt_cnr[dynamic_mask]\n",
        "            )\n",
        "        else:\n",
        "            loss_dynamic = torch.tensor(0.0, device=pred_cnr.device)\n",
        "\n",
        "        # 3: Temporal Delta Consistency\n",
        "        loss_delta = F.mse_loss(pred_delta, tgt_delta)\n",
        "\n",
        "        # 4: Variance Preservation (prevent collapse)\n",
        "        pred_std = pred_cnr.std()\n",
        "        tgt_std = tgt_cnr.std()\n",
        "        loss_var = torch.abs(pred_std - tgt_std)\n",
        "\n",
        "        # 5: Weighted Huber Loss\n",
        "        # Progressive scheduling\n",
        "        progress = min(self.current_epoch / self.max_epochs, 1.0)\n",
        "\n",
        "        # Squeeze for weight computation\n",
        "        cnr = prev_cnr.squeeze(-1)\n",
        "        target_d = tgt_delta.squeeze(-1)\n",
        "\n",
        "        # Dynamic magnitude weight -> Emphasize large changes progressively\n",
        "        beta = 1.5 + 0.5 * progress\n",
        "        dynamic_weight = torch.pow(torch.abs(target_d), beta)\n",
        "\n",
        "        # Normalize to [0, 1]\n",
        "        if dynamic_weight.max() > 0:\n",
        "            dynamic_weight = dynamic_weight / dynamic_weight.max()\n",
        "        else:\n",
        "            dynamic_weight = torch.zeros_like(dynamic_weight)\n",
        "\n",
        "        # Cloud region weight -> Moderate boost for high-CNR regions\n",
        "        cloud_threshold = 0.4\n",
        "        cloud_mask_w = (cnr > cloud_threshold).float()\n",
        "        cloud_excess = torch.clamp(cnr - cloud_threshold, min=0)\n",
        "        cloud_weight = 1.0 + self.cloud_boost * cloud_excess * cloud_mask_w\n",
        "\n",
        "        # Boundary weight\n",
        "        if radial_grad is not None:\n",
        "            grad = radial_grad.squeeze(-1)\n",
        "            is_boundary = (torch.abs(grad) > 0.3).float()\n",
        "            boundary_weight = 1.0 + 0.5 * is_boundary\n",
        "        else:\n",
        "            boundary_weight = 1.0\n",
        "\n",
        "        # Combine weights\n",
        "        alpha = 0.4 - 0.2 * progress  # 0.4 → 0.2\n",
        "\n",
        "        spatial_weight = cnr * cloud_weight * boundary_weight\n",
        "        combined_weight = alpha * spatial_weight + (1 - alpha) * dynamic_weight\n",
        "        combined_weight = torch.clamp(combined_weight, min=0.3, max=2.0)\n",
        "\n",
        "        # Apply to Huber loss\n",
        "        combined_weight = combined_weight.unsqueeze(-1)  # [B, N, 1]\n",
        "        loss_pointwise = self.huber(pred_delta, tgt_delta)\n",
        "        weighted_loss = loss_pointwise * combined_weight\n",
        "        loss_weighted_huber = weighted_loss.sum() / (combined_weight.sum() + 1e-6)\n",
        "\n",
        "        loss = (\n",
        "            0.15 * loss_mse +\n",
        "            0.30 * loss_dynamic +\n",
        "            0.15 * loss_delta +\n",
        "            0.05 * loss_var +\n",
        "            0.35 * loss_weighted_huber\n",
        "        )\n",
        "\n",
        "        # Loss components for logging\n",
        "        loss_dict = {\n",
        "            'total': loss.item(),\n",
        "            'mse': loss_mse.item(),\n",
        "            'dynamic': loss_dynamic.item() if isinstance(loss_dynamic, torch.Tensor) else 0.0,\n",
        "            'delta': loss_delta.item(),\n",
        "            'var': loss_var.item(),\n",
        "            'weighted_huber': loss_weighted_huber.item(),\n",
        "            'weight_mean': combined_weight.mean().item(),\n",
        "            'weight_max': combined_weight.max().item(),\n",
        "        }\n",
        "\n",
        "        return loss, loss_dict\n",
        "\n",
        "    def step_epoch(self):\n",
        "        \"\"\"Call at end of each epoch to update progressive scheduling\"\"\"\n",
        "        self.current_epoch += 1\n",
        "        print(f\"  → Loss scheduler: Epoch {self.current_epoch}/{self.max_epochs}, \"\n",
        "              f\"Progress: {self.current_epoch/self.max_epochs:.2%}\")\n"
      ],
      "metadata": {
        "id": "Y4lx4_aQSajJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Script"
      ],
      "metadata": {
        "id": "ijfDbmgUf-Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Start Run ##\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "lidar_dir = os.getcwd()\n",
        "weather_csv = \"weather_data_combined.csv\"\n",
        "T=6\n",
        "\n",
        "# Create sequence list from filenames\n",
        "files = sorted(os.listdir(os.path.join(lidar_dir, \"diff_clouds\")))\n",
        "datetimes = [datetime.strptime(f.split(\".\")[0], \"%Y-%m-%d_%H_%M\") for f in files]\n",
        "seqs = [datetimes[i:i+T+1] for i in range(len(datetimes)-T)]\n",
        "split = int(0.85 * len(seqs))\n",
        "train_seqs = seqs[:split]\n",
        "val_seqs = seqs[split:]\n",
        "\n",
        "train_dataset = WeatherEnhancedLTSMDataset(lidar_dir, train_seqs, weather_csv, T=T, use_polar=True)\n",
        "val_dataset = WeatherEnhancedLTSMDataset(lidar_dir, val_seqs, weather_csv, T=T, use_polar=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=1, shuffle=False)\n",
        "\n",
        "model = CloudLSTMNextScan(in_dim=9, hidden_dim=384, msg_dim=96, k=0, T=T).to(device, dtype=torch.float32)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "scaler = GradScaler()\n",
        "\n",
        "knn_file = os.path.join(lidar_dir, \"precomputed_neighbors/K32.pt\")\n",
        "if os.path.exists(knn_file):\n",
        "  print(\"Found file\")\n",
        "precomputed_indices_tensor = torch.load(knn_file, pickle_module=pickle)\n",
        "precomputed_indices_tensor = precomputed_indices_tensor.to(device=device, dtype=torch.long)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "# Optionally load from checkpoint\n",
        "start_epoch = 0\n",
        "base_path = \"Test7_LSTM_Epoch\"\n",
        "checkpoint_path = \"Test7_LSTM.pt\"\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device, pickle_module=pickle)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    print(f\"Resumed from checkpoint\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "starttime = time.time()\n",
        "criterion = ImprovedCloudDynamicsLoss(\n",
        "        max_epochs=num_epochs,\n",
        "        huber_delta=0.2,\n",
        "        cloud_boost=1.5,\n",
        "        dynamic_threshold=0.05\n",
        "    )\n",
        "\n",
        "print(f\"Control Group pass {checkpoint_path}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = total_empirical_bias = total_pred_change = 0.0\n",
        "    total_target_change = total_l1_loss = total_cloud_loss = total_cloud_loss1 = 0.0\n",
        "    total_bg_loss = total_bg_loss1 = pct_active = 0.0\n",
        "    total_l1_per = total_mse_loss = total_mse_per = 0.0\n",
        "    y = 0\n",
        "    epoch_components = {\n",
        "            'mse': 0.0,\n",
        "            'dynamic': 0.0,\n",
        "            'delta': 0.0,\n",
        "            'var': 0.0,\n",
        "            'weighted_huber': 0.0\n",
        "        }\n",
        "\n",
        "    for batch in train_loader:\n",
        "        # Check if batch dictionary is empty or missing expected keys\n",
        "        if not batch:\n",
        "            print(\"Skipping empty batch.\")\n",
        "            continue\n",
        "\n",
        "        # Get all point cloud keys dynamically\n",
        "        pc_keys = [key for key in batch.keys() if key.startswith('pc')]\n",
        "        pc_keys.sort()  # Ensure correct order\n",
        "\n",
        "        if len(pc_keys) < T + 1:  # Need T inputs + 1 target\n",
        "            print(\"Insufficient point clouds in batch.\")\n",
        "            continue\n",
        "\n",
        "        # Load point clouds to device\n",
        "        point_clouds = []\n",
        "        skip_batch = False\n",
        "\n",
        "        for key in pc_keys:\n",
        "            pc = batch[key].to(device)\n",
        "            if pc.size(1) == 0:  # Check for empty point clouds\n",
        "                print(f\"Empty point cloud found: {key}, skipping batch\")\n",
        "                skip_batch = True\n",
        "                break\n",
        "            point_clouds.append(pc)\n",
        "\n",
        "        if skip_batch:\n",
        "            y += 1\n",
        "            continue\n",
        "\n",
        "        # Split into inputs and target\n",
        "        input_pcs = point_clouds[:T]  # First T point clouds for input\n",
        "        target_pc = point_clouds[T]   # Last point cloud for target\n",
        "        # Stack the input point clouds: [B, T, N, F]\n",
        "        x = torch.stack(input_pcs, dim=1)\n",
        "\n",
        "        pc_current = point_clouds[T-1]\n",
        "        pc_target = target_pc\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with autocast(device_type='cuda', enabled=(device.type=='cuda')):\n",
        "            pred_next_feat, pred_delta = model(x, precomputed_indices_tensor)\n",
        "\n",
        "            if torch.isnan(pred_next_feat).any() or torch.isnan(pred_delta).any():\n",
        "                print(\"Nan in model output\")\n",
        "                continue\n",
        "\n",
        "            # Only consider CNR channel\n",
        "            prev_cnr = pc_current[:, :, 3:4]\n",
        "            tgt_delta = pc_target[:, :, 3:4] - prev_cnr\n",
        "            pred_cnr = pred_next_feat[:, :, 3:4]\n",
        "            tgt_cnr = pc_target[:, :, 3:4]\n",
        "            radial_grad = None\n",
        "\n",
        "            # Weighted loss\n",
        "            loss, loss_dict = criterion(\n",
        "                pred_cnr=pred_cnr,\n",
        "                tgt_cnr=tgt_cnr,\n",
        "                prev_cnr=prev_cnr,\n",
        "                radial_grad=radial_grad\n",
        "            )\n",
        "\n",
        "            if torch.isnan(loss):\n",
        "                print(\"NaN loss; skipping step.\")\n",
        "                continue\n",
        "\n",
        "        # Backward\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        for key in epoch_components:\n",
        "            epoch_components[key] += loss_dict[key]\n",
        "\n",
        "        # Metrics\n",
        "        with torch.no_grad():\n",
        "            # Global L1 loss\n",
        "            total_l1_loss += F.l1_loss(pred_cnr, tgt_cnr, reduction='mean')\n",
        "            total_l1_per += F.l1_loss(prev_cnr, tgt_cnr, reduction='mean')\n",
        "\n",
        "            # Global MSE loss\n",
        "            total_mse_loss += F.mse_loss(pred_cnr, tgt_cnr, reduction='mean')\n",
        "            total_mse_per += F.mse_loss(prev_cnr, tgt_cnr, reduction='mean')\n",
        "\n",
        "            # Changes relative to current\n",
        "            pred_change   = pred_cnr - prev_cnr\n",
        "            actual_change = tgt_cnr - prev_cnr\n",
        "            total_pred_change   += torch.abs(pred_change).mean()\n",
        "            total_target_change += torch.abs(actual_change).mean()\n",
        "\n",
        "            # Empirical bias\n",
        "            total_empirical_bias += (pred_delta.mean() - tgt_delta.mean()).item()\n",
        "\n",
        "            # Cloud vs. background split\n",
        "            cloud_mask = prev_cnr > 0.4\n",
        "            bg_mask    = prev_cnr <= 0.4\n",
        "\n",
        "            if cloud_mask.any():\n",
        "                cloud_loss = F.l1_loss(pred_cnr[cloud_mask], tgt_cnr[cloud_mask])\n",
        "                total_cloud_loss += cloud_loss.item()\n",
        "\n",
        "            if bg_mask.any():\n",
        "                bg_loss = F.l1_loss(pred_cnr[bg_mask], tgt_cnr[bg_mask])\n",
        "                total_bg_loss += bg_loss.item()\n",
        "\n",
        "            cloud_mask = tgt_cnr > 0.4\n",
        "            bg_mask    = tgt_cnr <= 0.4\n",
        "\n",
        "            if cloud_mask.any():\n",
        "                cloud_loss = F.l1_loss(pred_cnr[cloud_mask], tgt_cnr[cloud_mask])\n",
        "                total_cloud_loss1 += cloud_loss.item()\n",
        "\n",
        "            if bg_mask.any():\n",
        "                bg_loss = F.l1_loss(pred_cnr[bg_mask], tgt_cnr[bg_mask])\n",
        "                total_bg_loss1 += bg_loss.item()\n",
        "\n",
        "        # Debug Prints\n",
        "        if y % 400 == 0:\n",
        "            print(y)\n",
        "            with torch.no_grad():\n",
        "                p = pred_delta.view(-1).cpu()\n",
        "                t = tgt_delta.view(-1).cpu()\n",
        "\n",
        "                def show_stats(name, arr):\n",
        "                    arr = arr[torch.isfinite(arr)]\n",
        "                    if arr.numel() == 0:\n",
        "                        print(name, \"empty\")\n",
        "                        return\n",
        "                    print(f\"{name}: mean={arr.mean():.4f}, std={arr.std():.4f}, \"\n",
        "                          f\"p50={arr.median():.4f}, p90={arr.kthvalue(int(0.9*arr.numel()))[0]:.4f}, p95={arr.kthvalue(int(0.95*arr.numel()))[0]:.4f}\")\n",
        "\n",
        "                show_stats(\"pred_delta_std\", p)\n",
        "                show_stats(\"tgt_delta_std\", t)\n",
        "                print(f\"Target Delta stats: Max: {tgt_delta.max()} | Min {tgt_delta.min()} | Mean: {tgt_delta.mean()}\")\n",
        "                print(f\"Predicted Delta stats: Max: {pred_delta.max()} | Min {pred_delta.min()} | Mean: {pred_delta.mean()}\")\n",
        "                print(\"pct_nonzero_tgt (1dB+):\", (t.abs() > 0.02).float().mean().item(), flush=True)\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        if y % 5 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        del x, pred_next_feat, pred_delta, loss\n",
        "        y += 1\n",
        "\n",
        "    # Epoch Summary\n",
        "    n_batches = max(1, len(train_loader))\n",
        "    avg_loss = total_loss / n_batches\n",
        "    print(f\"[Epoch {epoch+1}] Train Average Total Loss: {total_loss / n_batches:.6f}\", flush=True)\n",
        "    print(f\" Empirical Bias: {total_empirical_bias / n_batches}\")\n",
        "    print(f\" Pred Change: {total_pred_change / n_batches} | Target Change: {total_target_change / n_batches}\")\n",
        "    print(f\" L1 Loss: {total_l1_loss / n_batches}, Persistance L1: {total_l1_per / n_batches}\")\n",
        "    print(f\" MSE Loss: {total_mse_loss / n_batches}, Persistance MSE: {total_mse_per / n_batches}\")\n",
        "    print(f\" Cloud Loss (prev cnr > 0.4): {total_cloud_loss / n_batches} | Background Loss (prev cnr < 0.4): {total_bg_loss / n_batches}\")\n",
        "    print(f\" Cloud Loss (tgt cnr > 0.4): {total_cloud_loss1 / n_batches} | Background Loss (tgt cnr < 0.4): {total_bg_loss1 / n_batches}\", flush=True)\n",
        "    print(f\"    - MSE (15%):           {epoch_components['mse']/n_batches:.6f}\")\n",
        "    print(f\"    - Dynamic (30%):       {epoch_components['dynamic']/n_batches:.6f}\")\n",
        "    print(f\"    - Delta (15%):         {epoch_components['delta']/n_batches:.6f}\")\n",
        "    print(f\"    - Variance (5%):       {epoch_components['var']/n_batches:.6f}\")\n",
        "    print(f\"    - Weighted Huber (35%): {epoch_components['weighted_huber']/n_batches:.6f}\")\n",
        "    criterion.step_epoch()\n",
        "\n",
        "    # Run Validation Set\n",
        "    model.eval()\n",
        "    results = {\n",
        "        'easy': {'model_mse': [], 'persist_mse': [], 'model_mae': [], 'persist_mae': [], 'corr': []},\n",
        "        'medium': {'model_mse': [], 'persist_mse': [], 'model_mae': [], 'persist_mae': [], 'corr': []},\n",
        "        'hard': {'model_mse': [], 'persist_mse': [], 'model_mae': [], 'persist_mae': [], 'corr': []},\n",
        "        'dynamic_region': {'model_mse': [], 'persist_mse': [], 'model_mae': [], 'persist_mae': []}\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            # Get point clouds\n",
        "            pc_keys = [key for key in batch.keys() if key.startswith('pc')]\n",
        "            pc_keys.sort()\n",
        "\n",
        "            if len(pc_keys) < T:\n",
        "                continue\n",
        "\n",
        "            point_clouds = []\n",
        "            for key in pc_keys:\n",
        "                pc = batch[key].to(device)\n",
        "                if pc.size(1) == 0:\n",
        "                    break\n",
        "                point_clouds.append(pc)\n",
        "\n",
        "            if len(point_clouds) < T+1:\n",
        "                continue\n",
        "\n",
        "            # Prepare inputs\n",
        "            input_pcs = point_clouds[:T]\n",
        "            x = torch.stack(input_pcs, dim=1)\n",
        "\n",
        "            pc_current = point_clouds[T-1]\n",
        "            pc_target = point_clouds[T]\n",
        "\n",
        "            # Get predictions\n",
        "            pred_next_feat, delta_cnr = model(x, precomputed_indices_tensor)\n",
        "            pred_cnr = pred_next_feat[:, :, 3:4]\n",
        "            tgt_cnr = pc_target[:, :, 3:4]\n",
        "            prev_cnr = pc_current[:, :, 3:4]\n",
        "\n",
        "            # Compute correlation (scene difficulty)\n",
        "            corr = torch.corrcoef(torch.stack([\n",
        "                prev_cnr.flatten(),\n",
        "                tgt_cnr.flatten()\n",
        "            ]))[0, 1].item()\n",
        "\n",
        "            # Compute metrics\n",
        "            model_mse = F.mse_loss(pred_cnr, tgt_cnr).item()\n",
        "            persist_mse = F.mse_loss(prev_cnr, tgt_cnr).item()\n",
        "            model_mae = F.l1_loss(pred_cnr, tgt_cnr).item()\n",
        "            persist_mae = F.l1_loss(prev_cnr, tgt_cnr).item()\n",
        "\n",
        "            # Categorize by difficulty\n",
        "            if corr > 0.85:\n",
        "                category = 'easy'\n",
        "            elif corr > 0.65:\n",
        "                category = 'medium'\n",
        "            else:\n",
        "                category = 'hard'\n",
        "\n",
        "            results[category]['model_mse'].append(model_mse)\n",
        "            results[category]['persist_mse'].append(persist_mse)\n",
        "            results[category]['model_mae'].append(model_mae)\n",
        "            results[category]['persist_mae'].append(persist_mae)\n",
        "            results[category]['corr'].append(corr)\n",
        "\n",
        "            # Evaluate on dynamic regions only\n",
        "            tgt_delta = tgt_cnr - prev_cnr\n",
        "            dynamic_mask = torch.abs(tgt_delta) > 0.05\n",
        "\n",
        "            if dynamic_mask.any():\n",
        "                model_mse_dyn = F.mse_loss(pred_cnr[dynamic_mask], tgt_cnr[dynamic_mask]).item()\n",
        "                persist_mse_dyn = F.mse_loss(prev_cnr[dynamic_mask], tgt_cnr[dynamic_mask]).item()\n",
        "                model_mae_dyn = F.l1_loss(pred_cnr[dynamic_mask], tgt_cnr[dynamic_mask]).item()\n",
        "                persist_mae_dyn = F.l1_loss(prev_cnr[dynamic_mask], tgt_cnr[dynamic_mask]).item()\n",
        "\n",
        "                results['dynamic_region']['model_mse'].append(model_mse_dyn)\n",
        "                results['dynamic_region']['persist_mse'].append(persist_mse_dyn)\n",
        "                results['dynamic_region']['model_mae'].append(model_mae_dyn)\n",
        "                results['dynamic_region']['persist_mae'].append(persist_mae_dyn)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STRATIFIED EVALUATION RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    for cat in ['easy', 'medium', 'hard']:\n",
        "        if len(results[cat]['model_mse']) == 0:\n",
        "            continue\n",
        "\n",
        "        model_mse = np.mean(results[cat]['model_mse'])\n",
        "        persist_mse = np.mean(results[cat]['persist_mse'])\n",
        "        model_mae = np.mean(results[cat]['model_mae'])\n",
        "        persist_mae = np.mean(results[cat]['persist_mae'])\n",
        "        avg_corr = np.mean(results[cat]['corr'])\n",
        "\n",
        "        mse_improvement = 100 * (persist_mse - model_mse) / persist_mse\n",
        "        mae_improvement = 100 * (persist_mae - model_mae) / persist_mae\n",
        "\n",
        "        print(f\"\\n{cat.upper()} SCENES (n={len(results[cat]['model_mse'])}, corr={avg_corr:.3f}):\")\n",
        "        print(f\"  MSE  - Model: {model_mse:.6f} | Persist: {persist_mse:.6f} | Δ: {mse_improvement:+.1f}%\")\n",
        "        print(f\"  MAE  - Model: {model_mae:.6f} | Persist: {persist_mae:.6f} | Δ: {mae_improvement:+.1f}%\")\n",
        "\n",
        "    # Dynamic region results\n",
        "    if len(results['dynamic_region']['model_mse']) > 0:\n",
        "        model_mse_dyn = np.mean(results['dynamic_region']['model_mse'])\n",
        "        persist_mse_dyn = np.mean(results['dynamic_region']['persist_mse'])\n",
        "        model_mae_dyn = np.mean(results['dynamic_region']['model_mae'])\n",
        "        persist_mae_dyn = np.mean(results['dynamic_region']['persist_mae'])\n",
        "\n",
        "        mse_improvement_dyn = 100 * (persist_mse_dyn - model_mse_dyn) / persist_mse_dyn\n",
        "        mae_improvement_dyn = 100 * (persist_mae_dyn - model_mae_dyn) / persist_mae_dyn\n",
        "\n",
        "        print(f\"\\nDYNAMIC REGIONS ONLY (|Δ| > 0.05, n={len(results['dynamic_region']['model_mse'])}):\")\n",
        "        print(f\"  MSE  - Model: {model_mse_dyn:.6f} | Persist: {persist_mse_dyn:.6f} | Δ: {mse_improvement_dyn:+.1f}%\")\n",
        "        print(f\"  MAE  - Model: {model_mae_dyn:.6f} | Persist: {persist_mae_dyn:.6f} | Δ: {mae_improvement_dyn:+.1f}%\")\n",
        "\n",
        "    print(\"=\"*70 + \"\\n\", flush=True)\n",
        "\n",
        "    # save overall model\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"loss\": avg_loss\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    # Save the epoch specific model\n",
        "    path = base_path + f\"{epoch}\" + \".pt\"\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"loss\": avg_loss\n",
        "    }, path)\n",
        "\n",
        "endtime = time.time()\n",
        "print(\"Timing was:\")\n",
        "print(endtime - starttime)"
      ],
      "metadata": {
        "id": "O0iPDyPmSal3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Precompute Radial Gradients"
      ],
      "metadata": {
        "id": "IVNqV0x5gLWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_radial_gradient_polar_numpy(pc):\n",
        "    \"\"\"\n",
        "    Compute radial gradients for a numpy point cloud\n",
        "\n",
        "    Args:\n",
        "        pc: [N, 7+] numpy array with [az, range, el, cnr, ...]\n",
        "\n",
        "    Returns:\n",
        "        radial_grad: [N] numpy array\n",
        "    \"\"\"\n",
        "    N = pc.shape[0]\n",
        "\n",
        "    azimuth = pc[:, 0]\n",
        "    range_m = pc[:, 1]\n",
        "    elevation = pc[:, 2]\n",
        "    cnr = pc[:, 3]\n",
        "\n",
        "    # Create beam IDs\n",
        "    az_bins = np.round(azimuth).astype(int)\n",
        "    el_bins = np.round(elevation).astype(int)\n",
        "    beam_id = (az_bins + 360) * 1000 + (el_bins + 90)\n",
        "\n",
        "    # Initialize output\n",
        "    radial_grad = np.zeros(N, dtype=np.float32)\n",
        "\n",
        "    # Process each beam\n",
        "    unique_beams = np.unique(beam_id)\n",
        "\n",
        "    for beam in unique_beams:\n",
        "        mask = beam_id == beam\n",
        "        indices = np.where(mask)[0]\n",
        "\n",
        "        if len(indices) < 2:\n",
        "            continue\n",
        "\n",
        "        # Sort by range\n",
        "        beam_ranges = range_m[indices]\n",
        "        beam_cnrs = cnr[indices]\n",
        "        sorted_idx = np.argsort(beam_ranges)\n",
        "\n",
        "        sorted_cnrs = beam_cnrs[sorted_idx]\n",
        "        orig_indices = indices[sorted_idx]\n",
        "\n",
        "        # Compute gradients\n",
        "        cnr_changes = np.diff(sorted_cnrs)\n",
        "\n",
        "        # Assign to points\n",
        "        radial_grad[orig_indices[:-1]] = cnr_changes\n",
        "        if len(cnr_changes) > 0:\n",
        "            radial_grad[orig_indices[-1]] = cnr_changes[-1]\n",
        "\n",
        "    return radial_grad\n",
        "\n",
        "\n",
        "def precompute_all_gradients(input_dir, output_dir, use_polar=True):\n",
        "    \"\"\"\n",
        "    Precompute gradients for all point clouds and save\n",
        "\n",
        "    Args:\n",
        "        input_dir: Directory with original point clouds\n",
        "        output_dir: Directory to save gradient files\n",
        "        use_polar: Whether using polar coordinates\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Get all files\n",
        "    if use_polar:\n",
        "        pc_dir = os.path.join(input_dir, 'diff_clouds_polar')\n",
        "        grad_dir = os.path.join(output_dir, 'gradients_polar')\n",
        "    else:\n",
        "        pc_dir = os.path.join(input_dir, 'diff_clouds')\n",
        "        grad_dir = os.path.join(output_dir, 'gradients')\n",
        "\n",
        "    os.makedirs(grad_dir, exist_ok=True)\n",
        "\n",
        "    files = [f for f in os.listdir(pc_dir) if f.endswith('.npy')]\n",
        "\n",
        "    print(f\"Precomputing gradients for {len(files)} point clouds...\")\n",
        "\n",
        "    for fname in tqdm(files):\n",
        "        # Load point cloud\n",
        "        pc_path = os.path.join(pc_dir, fname)\n",
        "        pc = np.load(pc_path)\n",
        "\n",
        "        if np.isnan(pc).any():\n",
        "            pc = np.nan_to_num(pc)\n",
        "\n",
        "        pc = pc[pc[:, 1] <= 0.2 * 14500]\n",
        "\n",
        "        if pc.shape[0] == 0:\n",
        "            print(\"Deleted all points\")\n",
        "\n",
        "        # Compute gradient\n",
        "        radial_grad = compute_radial_gradient_polar_numpy(pc)\n",
        "\n",
        "        # Save gradient (same filename)\n",
        "        grad_path = os.path.join(grad_dir, fname)\n",
        "        np.save(grad_path, radial_grad)\n",
        "\n",
        "    print(f\"Saved gradients to {grad_dir}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run precomputation\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    root = os.getcwd()\n",
        "    lidar_dir = os.path.join(root, 'LiDAR')\n",
        "\n",
        "    precompute_all_gradients(\n",
        "        input_dir = lidar_dir,\n",
        "        output_dir = lidar_dir,\n",
        "        use_polar=True\n",
        "    )"
      ],
      "metadata": {
        "id": "3NlcgkD8Saom"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}